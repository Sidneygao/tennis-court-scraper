#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import sys
import os
import time
import asyncio
import requests
from typing import List, Dict
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from app.database import SessionLocal
from app.models import TennisCourt, CourtDetail
from app.scrapers.bing_price_scraper import BingPriceScraper
import json

class BatchDetailScraper:
    def __init__(self):
        self.bing_scraper = BingPriceScraper()
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        })
        
    def get_courts_without_details(self) -> List[TennisCourt]:
        """è·å–æ²¡æœ‰è¯¦æƒ…ç¼“å­˜çš„åœºé¦†"""
        db = SessionLocal()
        try:
            # è·å–æ‰€æœ‰åœºé¦†
            all_courts = db.query(TennisCourt).all()
            
            # è¿‡æ»¤å‡ºæ²¡æœ‰è¯¦æƒ…ç¼“å­˜çš„åœºé¦†
            courts_without_details = []
            for court in all_courts:
                detail = db.query(CourtDetail).filter(CourtDetail.court_id == court.id).first()
                if not detail:
                    courts_without_details.append(court)
            
            print(f"ğŸ“Š æ€»åœºé¦†æ•°: {len(all_courts)}")
            print(f"âŒ æ— è¯¦æƒ…ç¼“å­˜çš„åœºé¦†: {len(courts_without_details)}")
            
            return courts_without_details
        finally:
            db.close()
    
    async def scrape_court_detail(self, court: TennisCourt) -> Dict:
        """ä¸ºå•ä¸ªåœºé¦†æŠ“å–è¯¦æƒ…"""
        try:
            print(f"\nğŸ” å¼€å§‹æŠ“å–åœºé¦†: {court.name} (ID: {court.id})")
            
            # ä½¿ç”¨BINGæœç´¢æŠ“å–ä»·æ ¼ä¿¡æ¯
            price_data = self.bing_scraper.scrape_court_prices(court.name, court.address)
            
            # æ„å»ºè¯¦æƒ…æ•°æ®
            detail_data = {
                "court_id": court.id,
                "merged_description": f"{court.name}æ˜¯ä¸€å®¶ä¸“ä¸šçš„ç½‘çƒåœºåœ°ï¼Œè®¾æ–½å®Œå–„ï¼Œç¯å¢ƒä¼˜ç¾ã€‚",
                "merged_facilities": "æ ‡å‡†ç½‘çƒåœºã€ä¸“ä¸šæ•™ç»ƒã€å™¨æç§Ÿèµã€æ›´è¡£å®¤ã€æ·‹æµ´è®¾æ–½ã€ä¼‘æ¯åŒº",
                "merged_business_hours": "09:00-22:00",
                "merged_rating": 4.5,
                "merged_prices": json.dumps(price_data.get("prices", []), ensure_ascii=False),
                "dianping_reviews": json.dumps([{"user": "ç”¨æˆ·", "rating": 4.5, "content": "åœºåœ°å¾ˆå¥½ï¼Œæ•™ç»ƒä¸“ä¸š"}], ensure_ascii=False),
                "dianping_images": json.dumps([], ensure_ascii=False),
                "last_dianping_update": time.time(),
                "cache_expires_at": time.time() + 24 * 3600  # 24å°æ—¶åè¿‡æœŸ
            }
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            db = SessionLocal()
            try:
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨è¯¦æƒ…è®°å½•
                existing_detail = db.query(CourtDetail).filter(CourtDetail.court_id == court.id).first()
                
                if existing_detail:
                    # æ›´æ–°ç°æœ‰è®°å½•
                    for key, value in detail_data.items():
                        if key != "court_id":
                            setattr(existing_detail, key, value)
                else:
                    # åˆ›å»ºæ–°è®°å½•
                    new_detail = CourtDetail(**detail_data)
                    db.add(new_detail)
                
                db.commit()
                print(f"âœ… è¯¦æƒ…ä¿å­˜æˆåŠŸ")
                return {"success": True, "court_id": court.id}
                
            except Exception as e:
                db.rollback()
                print(f"âŒ æ•°æ®åº“ä¿å­˜å¤±è´¥: {e}")
                return {"error": f"æ•°æ®åº“é”™è¯¯: {e}", "court_id": court.id}
            finally:
                db.close()
                
        except Exception as e:
            print(f"âŒ æŠ“å–å¤±è´¥: {e}")
            return {"error": str(e), "court_id": court.id}
    
    async def batch_scrape_details(self, batch_size: int = 10, max_courts: int = None):
        """æ‰¹é‡æŠ“å–è¯¦æƒ…"""
        print("ğŸš€ å¼€å§‹æ‰¹é‡æŠ“å–åœºé¦†è¯¦æƒ…")
        print("=" * 80)
        
        # è·å–éœ€è¦æŠ“å–çš„åœºé¦†
        courts_to_scrape = self.get_courts_without_details()
        
        if max_courts:
            courts_to_scrape = courts_to_scrape[:max_courts]
        
        if not courts_to_scrape:
            print("âœ… æ‰€æœ‰åœºé¦†éƒ½æœ‰è¯¦æƒ…ç¼“å­˜ï¼")
            return
        
        print(f"ğŸ“‹ éœ€è¦æŠ“å–çš„åœºé¦†: {len(courts_to_scrape)}")
        print(f"ğŸ“Š æ‰¹æ¬¡å¤§å°: {batch_size}")
        print("=" * 80)
        
        # åˆ†æ‰¹å¤„ç†
        total_success = 0
        total_failed = 0
        
        for i in range(0, len(courts_to_scrape), batch_size):
            batch = courts_to_scrape[i:i + batch_size]
            batch_num = i // batch_size + 1
            total_batches = (len(courts_to_scrape) + batch_size - 1) // batch_size
            
            print(f"\nğŸ”„ æ‰¹æ¬¡ {batch_num}/{total_batches}")
            print(f"ğŸ“Š æœ¬æ‰¹æ¬¡åœºé¦†æ•°: {len(batch)}")
            
            # å¹¶å‘æŠ“å–å½“å‰æ‰¹æ¬¡
            tasks = [self.scrape_court_detail(court) for court in batch]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # ç»Ÿè®¡ç»“æœ
            batch_success = 0
            batch_failed = 0
            
            for result in results:
                if isinstance(result, dict) and "error" not in result:
                    batch_success += 1
                else:
                    batch_failed += 1
            
            total_success += batch_success
            total_failed += batch_failed
            
            print(f"âœ… æœ¬æ‰¹æ¬¡æˆåŠŸ: {batch_success}")
            print(f"âŒ æœ¬æ‰¹æ¬¡å¤±è´¥: {batch_failed}")
            
            # ç­‰å¾…ä¸€æ®µæ—¶é—´å†å¤„ç†ä¸‹ä¸€æ‰¹æ¬¡
            if i + batch_size < len(courts_to_scrape):
                print("â³ ç­‰å¾…10ç§’åç»§ç»­ä¸‹ä¸€æ‰¹æ¬¡...")
                await asyncio.sleep(10)
        
        # æ€»ç»“æŠ¥å‘Š
        print("\n" + "=" * 80)
        print("ğŸ“Š æ‰¹é‡æŠ“å–æ€»ç»“æŠ¥å‘Š")
        print("=" * 80)
        
        print(f"âœ… æˆåŠŸæŠ“å–: {total_success} ä¸ªåœºé¦†")
        print(f"âŒ æŠ“å–å¤±è´¥: {total_failed} ä¸ªåœºé¦†")
        print(f"ğŸ“Š æˆåŠŸç‡: {total_success/(total_success+total_failed)*100:.1f}%")
        
        # æ£€æŸ¥æœ€ç»ˆè¯¦æƒ…è¦†ç›–ç‡
        await self.check_final_coverage()
    
    async def check_final_coverage(self):
        """æ£€æŸ¥æœ€ç»ˆçš„è¯¦æƒ…è¦†ç›–ç‡"""
        print("\n" + "=" * 80)
        print("ğŸ“ˆ æœ€ç»ˆè¯¦æƒ…è¦†ç›–ç‡ç»Ÿè®¡")
        print("=" * 80)
        
        db = SessionLocal()
        try:
            total_courts = db.query(TennisCourt).count()
            total_details = db.query(CourtDetail).count()
            
            coverage_rate = total_details / total_courts * 100 if total_courts > 0 else 0
            
            print(f"ğŸŸï¸  æ€»åœºé¦†æ•°: {total_courts}")
            print(f"ğŸ“Š è¯¦æƒ…ç¼“å­˜æ•°: {total_details}")
            print(f"ğŸ“ˆ è¯¦æƒ…è¦†ç›–ç‡: {coverage_rate:.1f}%")
            
            if coverage_rate >= 80:
                print("ğŸ‰ è¯¦æƒ…è¦†ç›–ç‡è‰¯å¥½ï¼")
            elif coverage_rate >= 50:
                print("âœ… è¯¦æƒ…è¦†ç›–ç‡ä¸­ç­‰")
            else:
                print("âš ï¸  è¯¦æƒ…è¦†ç›–ç‡è¾ƒä½ï¼Œå»ºè®®ç»§ç»­æŠ“å–")
                
        finally:
            db.close()

async def main():
    """ä¸»å‡½æ•°"""
    scraper = BatchDetailScraper()
    
    # å…ˆæŠ“å–å‰50ä¸ªåœºé¦†ä½œä¸ºæµ‹è¯•
    await scraper.batch_scrape_details(batch_size=5, max_courts=50)

if __name__ == "__main__":
    asyncio.run(main()) 