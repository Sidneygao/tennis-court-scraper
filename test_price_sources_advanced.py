#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é«˜çº§ç½‘çƒåœºé¦†ä»·æ ¼æŠ“å–æµ‹è¯• - ä½¿ç”¨æ›´æ™ºèƒ½çš„ååçˆ¬æŠ€æœ¯
"""

import requests
import time
import json
import re
from bs4 import BeautifulSoup
from urllib.parse import quote, urljoin
import random

class AdvancedPriceScraperTest:
    def __init__(self):
        self.session = requests.Session()
        # ä½¿ç”¨éšæœºUA
        user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        
        self.session.headers.update({
            'User-Agent': user_agent,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0',
        })
        
    def extract_prices_from_text(self, text):
        """ä»æ–‡æœ¬ä¸­æå–ä»·æ ¼ä¿¡æ¯"""
        # æ›´ç²¾ç¡®çš„ä»·æ ¼æ­£åˆ™è¡¨è¾¾å¼
        price_patterns = [
            r'(\d+)[\s\-]*å…ƒ/?å°æ—¶?',
            r'(\d+)[\s\-]*å…ƒ/?åœº',
            r'(\d+)[\s\-]*å…ƒ/?æ¬¡',
            r'(\d+)[\s\-]*å…ƒ/?äºº',
            r'ä»·æ ¼[ï¼š:]\s*(\d+)[\s\-]*å…ƒ',
            r'æ”¶è´¹[ï¼š:]\s*(\d+)[\s\-]*å…ƒ',
            r'(\d+)[\s\-]*å…ƒ/?å°æ—¶?',
            r'(\d+)[\s\-]*å…ƒ/?åœº',
        ]
        
        prices = []
        for pattern in price_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            prices.extend(matches)
        
        return list(set(prices))  # å»é‡
    
    def test_ledongli_advanced(self, court_name):
        """æµ‹è¯•ä¹åŠ¨åŠ›ä»·æ ¼æŠ“å– - é«˜çº§ç‰ˆæœ¬"""
        print(f"\nğŸ” æµ‹è¯•ä¹åŠ¨åŠ›: {court_name}")
        try:
            # å°è¯•å¤šä¸ªæœç´¢å…³é”®è¯
            search_keywords = [
                f"{court_name}ç½‘çƒ",
                f"{court_name}ä»·æ ¼",
                f"{court_name}æ”¶è´¹",
                "ç½‘çƒ" + court_name
            ]
            
            for keyword in search_keywords:
                search_url = f"https://www.ledongli.cn/search?keyword={quote(keyword)}"
                print(f"å°è¯•æœç´¢: {keyword}")
                
                response = self.session.get(search_url, timeout=15)
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # æå–æ‰€æœ‰æ–‡æœ¬å†…å®¹
                    text_content = soup.get_text()
                    prices = self.extract_prices_from_text(text_content)
                    
                    if prices:
                        print(f"âœ… æ‰¾åˆ°ä»·æ ¼ä¿¡æ¯: {prices}")
                        return prices
                    
                    # æŸ¥æ‰¾ç‰¹å®šå…ƒç´ 
                    price_elements = soup.find_all(string=re.compile(r'\d+å…ƒ'))
                    if price_elements:
                        prices = [elem.strip() for elem in price_elements if 'å…ƒ' in elem]
                        print(f"âœ… æ‰¾åˆ°ä»·æ ¼å…ƒç´ : {prices[:3]}")
                        return prices[:3]
                
                time.sleep(random.uniform(2, 4))
            
            print("âŒ æœªæ‰¾åˆ°ä»·æ ¼ä¿¡æ¯")
            return None
                
        except Exception as e:
            print(f"âŒ ä¹åŠ¨åŠ›æŠ“å–å¼‚å¸¸: {e}")
            return None
    
    def test_quyundong_advanced(self, court_name):
        """æµ‹è¯•è¶£è¿åŠ¨ä»·æ ¼æŠ“å– - é«˜çº§ç‰ˆæœ¬"""
        print(f"\nğŸ” æµ‹è¯•è¶£è¿åŠ¨: {court_name}")
        try:
            # è¶£è¿åŠ¨å¯èƒ½ä½¿ç”¨ä¸åŒçš„URLç»“æ„
            search_urls = [
                f"https://www.quyundong.com/search?q={quote(court_name + 'ç½‘çƒ')}",
                f"https://www.quyundong.com/search?keyword={quote(court_name)}",
                f"https://www.quyundong.com/venue/search?q={quote(court_name + 'ç½‘çƒ')}"
            ]
            
            for search_url in search_urls:
                print(f"å°è¯•URL: {search_url}")
                
                response = self.session.get(search_url, timeout=15)
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # æå–æ‰€æœ‰æ–‡æœ¬å†…å®¹
                    text_content = soup.get_text()
                    prices = self.extract_prices_from_text(text_content)
                    
                    if prices:
                        print(f"âœ… æ‰¾åˆ°ä»·æ ¼ä¿¡æ¯: {prices}")
                        return prices
                
                time.sleep(random.uniform(2, 4))
            
            print("âŒ æœªæ‰¾åˆ°ä»·æ ¼ä¿¡æ¯")
            return None
                
        except Exception as e:
            print(f"âŒ è¶£è¿åŠ¨æŠ“å–å¼‚å¸¸: {e}")
            return None
    
    def test_douban_advanced(self, court_name):
        """æµ‹è¯•è±†ç“£ä»·æ ¼æŠ“å– - é«˜çº§ç‰ˆæœ¬"""
        print(f"\nğŸ” æµ‹è¯•è±†ç“£: {court_name}")
        try:
            # è±†ç“£æœç´¢
            search_url = f"https://www.douban.com/search?cat=1005&q={quote(court_name + 'ç½‘çƒ')}"
            print(f"æœç´¢URL: {search_url}")
            
            response = self.session.get(search_url, timeout=15)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # æå–æ‰€æœ‰æ–‡æœ¬å†…å®¹
                text_content = soup.get_text()
                prices = self.extract_prices_from_text(text_content)
                
                if prices:
                    print(f"âœ… æ‰¾åˆ°ä»·æ ¼ä¿¡æ¯: {prices}")
                    return prices
                
                # æŸ¥æ‰¾ç‰¹å®šå…ƒç´ 
                price_elements = soup.find_all(string=re.compile(r'\d+å…ƒ'))
                if price_elements:
                    prices = [elem.strip() for elem in price_elements if 'å…ƒ' in elem]
                    print(f"âœ… æ‰¾åˆ°ä»·æ ¼å…ƒç´ : {prices[:3]}")
                    return prices[:3]
            else:
                print(f"âŒ è¯·æ±‚å¤±è´¥: {response.status_code}")
            
            print("âŒ æœªæ‰¾åˆ°ä»·æ ¼ä¿¡æ¯")
            return None
                
        except Exception as e:
            print(f"âŒ è±†ç“£æŠ“å–å¼‚å¸¸: {e}")
            return None
    
    def test_zhihu_advanced(self, court_name):
        """æµ‹è¯•çŸ¥ä¹ä»·æ ¼æŠ“å– - é«˜çº§ç‰ˆæœ¬"""
        print(f"\nğŸ” æµ‹è¯•çŸ¥ä¹: {court_name}")
        try:
            # çŸ¥ä¹æœç´¢ - å°è¯•ä¸åŒçš„æœç´¢ç­–ç•¥
            search_urls = [
                f"https://www.zhihu.com/search?type=content&q={quote(court_name + 'ç½‘çƒä»·æ ¼')}",
                f"https://www.zhihu.com/search?type=content&q={quote(court_name + 'æ”¶è´¹')}",
                f"https://www.zhihu.com/search?type=content&q={quote('åŒ—äº¬' + court_name + 'ç½‘çƒ')}"
            ]
            
            for search_url in search_urls:
                print(f"å°è¯•URL: {search_url}")
                
                response = self.session.get(search_url, timeout=15)
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # æå–æ‰€æœ‰æ–‡æœ¬å†…å®¹
                    text_content = soup.get_text()
                    prices = self.extract_prices_from_text(text_content)
                    
                    if prices:
                        print(f"âœ… æ‰¾åˆ°ä»·æ ¼ä¿¡æ¯: {prices}")
                        return prices
                else:
                    print(f"âŒ è¯·æ±‚å¤±è´¥: {response.status_code}")
                
                time.sleep(random.uniform(2, 4))
            
            print("âŒ æœªæ‰¾åˆ°ä»·æ ¼ä¿¡æ¯")
            return None
                
        except Exception as e:
            print(f"âŒ çŸ¥ä¹æŠ“å–å¼‚å¸¸: {e}")
            return None
    
    def test_baidu_search(self, court_name):
        """æµ‹è¯•ç™¾åº¦æœç´¢ä»·æ ¼æŠ“å–"""
        print(f"\nğŸ” æµ‹è¯•ç™¾åº¦æœç´¢: {court_name}")
        try:
            search_url = f"https://www.baidu.com/s?wd={quote(court_name + 'ç½‘çƒä»·æ ¼æ”¶è´¹')}"
            print(f"æœç´¢URL: {search_url}")
            
            response = self.session.get(search_url, timeout=15)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # æå–æ‰€æœ‰æ–‡æœ¬å†…å®¹
                text_content = soup.get_text()
                prices = self.extract_prices_from_text(text_content)
                
                if prices:
                    print(f"âœ… æ‰¾åˆ°ä»·æ ¼ä¿¡æ¯: {prices}")
                    return prices
            
            print("âŒ æœªæ‰¾åˆ°ä»·æ ¼ä¿¡æ¯")
            return None
                
        except Exception as e:
            print(f"âŒ ç™¾åº¦æœç´¢æŠ“å–å¼‚å¸¸: {e}")
            return None
    
    def test_all_platforms_advanced(self, court_name):
        """æµ‹è¯•æ‰€æœ‰å¹³å° - é«˜çº§ç‰ˆæœ¬"""
        print(f"\nğŸ¾ å¼€å§‹æµ‹è¯•åœºé¦†: {court_name}")
        print("=" * 50)
        
        results = {}
        
        # æµ‹è¯•ä¹åŠ¨åŠ›
        results['ledongli'] = self.test_ledongli_advanced(court_name)
        time.sleep(random.uniform(3, 5))
        
        # æµ‹è¯•è¶£è¿åŠ¨
        results['quyundong'] = self.test_quyundong_advanced(court_name)
        time.sleep(random.uniform(3, 5))
        
        # æµ‹è¯•è±†ç“£
        results['douban'] = self.test_douban_advanced(court_name)
        time.sleep(random.uniform(3, 5))
        
        # æµ‹è¯•çŸ¥ä¹
        results['zhihu'] = self.test_zhihu_advanced(court_name)
        time.sleep(random.uniform(3, 5))
        
        # æµ‹è¯•ç™¾åº¦æœç´¢
        results['baidu'] = self.test_baidu_search(court_name)
        time.sleep(random.uniform(3, 5))
        
        # æ±‡æ€»ç»“æœ
        print(f"\nğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»:")
        print("=" * 50)
        for platform, result in results.items():
            status = "âœ… æˆåŠŸ" if result else "âŒ å¤±è´¥"
            print(f"{platform:12}: {status}")
            if result:
                print(f"           ä»·æ ¼: {result}")
        
        return results

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¾ é«˜çº§ç½‘çƒåœºé¦†ä»·æ ¼æŠ“å–æµ‹è¯•")
    print("=" * 50)
    
    # æµ‹è¯•åœºé¦†åˆ—è¡¨
    test_courts = [
        "å˜‰é‡Œä¸­å¿ƒç½‘çƒåœº",
        "é‡‘åœ°ç½‘çƒä¸­å¿ƒ", 
        "WoowTennisç½‘çƒä¿±ä¹éƒ¨"
    ]
    
    scraper = AdvancedPriceScraperTest()
    
    all_results = {}
    
    for court in test_courts:
        results = scraper.test_all_platforms_advanced(court)
        all_results[court] = results
        print("\n" + "=" * 60 + "\n")
    
    # ä¿å­˜æµ‹è¯•ç»“æœ
    with open('price_test_results_advanced.json', 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    
    print("ğŸ“ æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ° price_test_results_advanced.json")
    
    # ç»Ÿè®¡æˆåŠŸç‡
    total_tests = len(test_courts) * 5  # 5ä¸ªå¹³å°
    successful_tests = sum(1 for court_results in all_results.values() 
                          for result in court_results.values() if result)
    
    print(f"\nğŸ“ˆ æµ‹è¯•ç»Ÿè®¡:")
    print(f"æ€»æµ‹è¯•æ•°: {total_tests}")
    print(f"æˆåŠŸæ•°: {successful_tests}")
    print(f"æˆåŠŸç‡: {successful_tests/total_tests*100:.1f}%")

if __name__ == "__main__":
    main() 