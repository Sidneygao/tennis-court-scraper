#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–ç‰ˆ Google Monica AI å…³é”®è¯æœç´¢æµ‹è¯•
æ”¯æŒGoogleã€Bingã€DuckDuckGoç­‰å¤šä¸ªæœç´¢å¼•æ“
é…ç½®Clashä»£ç†æ”¯æŒ
"""

import requests
import time
import json
import re
from bs4 import BeautifulSoup
from urllib.parse import quote
import random

def test_google_monica_search(keyword):
    """æµ‹è¯•Google Monica AIé£æ ¼çš„å…³é”®è¯æœç´¢"""
    print(f"ğŸ¤– Google Monica AI é£æ ¼æœç´¢æµ‹è¯•")
    print(f"ğŸ” å…³é”®è¯: {keyword}")
    print("=" * 60)
    
    # è®¾ç½®è¯·æ±‚å¤´
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    }
    
    # Clashä»£ç†é…ç½® - ç«¯å£1086
    proxies = {
        "http": "socks5h://127.0.0.1:1086",
        "https": "socks5h://127.0.0.1:1086"
    }
    
    print(f"ğŸ”§ ä½¿ç”¨Clashä»£ç†: socks5h://127.0.0.1:1086")
    
    # ä»·æ ¼æå–æ­£åˆ™è¡¨è¾¾å¼
    price_patterns = [
        r'(\d+)\s*å…ƒ/?å°æ—¶',
        r'(\d+)\s*å…ƒ/?åœº',
        r'ä»·æ ¼[ï¼š:]\s*(\d+)\s*å…ƒ',
        r'æ”¶è´¹[ï¼š:]\s*(\d+)\s*å…ƒ',
        r'å¹³æ—¥ä»·[ï¼š:]\s*(\d+)\s*å…ƒ',
        r'å‘¨æœ«ä»·[ï¼š:]\s*(\d+)\s*å…ƒ',
        r'(\d+)\s*-\s*(\d+)\s*å…ƒ',
        r'(\d+)\s*åˆ°\s*(\d+)\s*å…ƒ',
        r'ï¿¥(\d+)',
        r'(\d+)\s*å—',
        r'(\d+)\s*å…ƒèµ·',
        r'(\d+)\s*å…ƒ/äºº',
        r'(\d+)\s*å…ƒ/æ¬¡'
    ]
    
    def extract_prices(text):
        """ä»æ–‡æœ¬ä¸­æå–ä»·æ ¼"""
        prices = []
        for pattern in price_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    if len(match) == 2:
                        prices.append(f"{match[0]}-{match[1]}å…ƒ")
                    else:
                        prices.append(f"{match[0]}å…ƒ")
                else:
                    prices.append(f"{match}å…ƒ")
        return list(set(prices))
    
    # æœç´¢å¼•æ“åˆ—è¡¨
    search_engines = [
        {
            'name': 'Google',
            'url': f'https://www.google.com/search?q={quote(keyword)}&num=5&hl=zh-CN',
            'parser': 'google'
        },
        {
            'name': 'Bing',
            'url': f'https://www.bing.com/search?q={quote(keyword)}&count=5',
            'parser': 'bing'
        },
        {
            'name': 'DuckDuckGo',
            'url': f'https://duckduckgo.com/html/?q={quote(keyword)}',
            'parser': 'duckduckgo'
        }
    ]
    
    all_results = []
    
    for engine in search_engines:
        print(f"\nğŸ” å°è¯• {engine['name']} æœç´¢...")
        
        try:
            # æ·»åŠ å»¶è¿Ÿ
            time.sleep(random.uniform(2, 4))
            
            # ä½¿ç”¨ä»£ç†å‘é€è¯·æ±‚
            response = requests.get(engine['url'], headers=headers, proxies=proxies, timeout=15)
            response.raise_for_status()
            
            print(f"âœ… {engine['name']} æœç´¢æˆåŠŸ")
            
            # è§£ææœç´¢ç»“æœ
            soup = BeautifulSoup(response.text, 'html.parser')
            
            if engine['parser'] == 'google':
                # Googleæœç´¢ç»“æœè§£æ
                results = soup.find_all('div', class_='g')
                for i, result in enumerate(results[:5], 1):
                    try:
                        title_elem = result.find('h3')
                        title = title_elem.get_text().strip() if title_elem else "æ— æ ‡é¢˜"
                        
                        link_elem = result.find('a')
                        url = link_elem.get('href') if link_elem else ""
                        
                        snippet_elem = result.find('div', class_='VwiC3b')
                        snippet = snippet_elem.get_text().strip() if snippet_elem else ""
                        
                        if title and url and url.startswith('http'):
                            # æå–ä»·æ ¼
                            snippet_prices = extract_prices(snippet)
                            
                            all_results.append({
                                'engine': engine['name'],
                                'rank': i,
                                'title': title,
                                'url': url,
                                'snippet': snippet,
                                'prices': snippet_prices
                            })
                            
                            print(f"  ğŸ“„ {i}. {title}")
                            print(f"     ğŸ’° ä»·æ ¼: {snippet_prices}")
                            
                    except Exception as e:
                        print(f"  âš ï¸ è§£æç»“æœå‡ºé”™: {e}")
                        continue
            
            elif engine['parser'] == 'bing':
                # Bingæœç´¢ç»“æœè§£æ
                results = soup.find_all('li', class_='b_algo')
                for i, result in enumerate(results[:5], 1):
                    try:
                        title_elem = result.find('h2')
                        title = title_elem.get_text().strip() if title_elem else "æ— æ ‡é¢˜"
                        
                        link_elem = result.find('a')
                        url = link_elem.get('href') if link_elem else ""
                        
                        snippet_elem = result.find('p')
                        snippet = snippet_elem.get_text().strip() if snippet_elem else ""
                        
                        if title and url:
                            snippet_prices = extract_prices(snippet)
                            
                            all_results.append({
                                'engine': engine['name'],
                                'rank': i,
                                'title': title,
                                'url': url,
                                'snippet': snippet,
                                'prices': snippet_prices
                            })
                            
                            print(f"  ğŸ“„ {i}. {title}")
                            print(f"     ğŸ’° ä»·æ ¼: {snippet_prices}")
                            
                    except Exception as e:
                        print(f"  âš ï¸ è§£æç»“æœå‡ºé”™: {e}")
                        continue
            
            elif engine['parser'] == 'duckduckgo':
                # DuckDuckGoæœç´¢ç»“æœè§£æ
                results = soup.find_all('div', class_='result')
                for i, result in enumerate(results[:5], 1):
                    try:
                        title_elem = result.find('a', class_='result__a')
                        title = title_elem.get_text().strip() if title_elem else "æ— æ ‡é¢˜"
                        
                        url = title_elem.get('href') if title_elem else ""
                        
                        snippet_elem = result.find('a', class_='result__snippet')
                        snippet = snippet_elem.get_text().strip() if snippet_elem else ""
                        
                        if title and url:
                            snippet_prices = extract_prices(snippet)
                            
                            all_results.append({
                                'engine': engine['name'],
                                'rank': i,
                                'title': title,
                                'url': url,
                                'snippet': snippet,
                                'prices': snippet_prices
                            })
                            
                            print(f"  ğŸ“„ {i}. {title}")
                            print(f"     ğŸ’° ä»·æ ¼: {snippet_prices}")
                            
                    except Exception as e:
                        print(f"  âš ï¸ è§£æç»“æœå‡ºé”™: {e}")
                        continue
                        
        except requests.exceptions.RequestException as e:
            print(f"âŒ {engine['name']} æœç´¢å¤±è´¥: {e}")
            continue
    
    # æ±‡æ€»ç»“æœ
    all_prices = []
    for result in all_results:
        all_prices.extend(result['prices'])
    
    summary = {
        'keyword': keyword,
        'search_time': time.strftime('%Y-%m-%d %H:%M:%S'),
        'total_results': len(all_results),
        'total_prices_found': len(all_prices),
        'unique_prices': list(set(all_prices)),
        'results_by_engine': {},
        'proxy_used': 'socks5h://127.0.0.1:1086'
    }
    
    # æŒ‰æœç´¢å¼•æ“åˆ†ç»„
    for result in all_results:
        engine = result['engine']
        if engine not in summary['results_by_engine']:
            summary['results_by_engine'][engine] = []
        summary['results_by_engine'][engine].append(result)
    
    print(f"\nğŸ“Š æœç´¢ç»“æœæ±‡æ€»:")
    print(f"  æ€»ç»“æœæ•°: {summary['total_results']}")
    print(f"  æ‰¾åˆ°ä»·æ ¼: {summary['total_prices_found']} ä¸ª")
    print(f"  å”¯ä¸€ä»·æ ¼: {summary['unique_prices']}")
    
    return summary

if __name__ == "__main__":
    # æµ‹è¯•å…³é”®è¯
    keyword = "æœé˜³å…¬å›­ç½‘çƒåœº ä»·æ ¼ 2024"
    
    result = test_google_monica_search(keyword)
    
    # ä¿å­˜ç»“æœ
    filename = f"google_monica_clash_{keyword.replace(' ', '_')}_{time.strftime('%Y%m%d_%H%M%S')}.json"
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {filename}") 