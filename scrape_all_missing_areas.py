#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import sys
import os
import time
import asyncio
import requests
from typing import List, Dict
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from app.database import SessionLocal
from app.models import TennisCourt
from app.config import settings

class MissingAreasScraper:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
    def get_missing_areas(self) -> List[str]:
        """è·å–ç¼ºå¤±çš„åŒºåŸŸåˆ—è¡¨"""
        db = SessionLocal()
        try:
            # è·å–å½“å‰å·²æœ‰çš„åŒºåŸŸ
            existing_areas = db.query(TennisCourt.area).distinct().all()
            existing_areas = [area[0] for area in existing_areas if area[0]]
            
            # æ‰¾å‡ºç¼ºå¤±çš„åŒºåŸŸ
            missing_areas = [area for area in settings.target_areas if area not in existing_areas]
            
            print(f"ğŸ“Š å½“å‰å·²æœ‰åŒºåŸŸ: {existing_areas}")
            print(f"âŒ ç¼ºå¤±çš„åŒºåŸŸ: {missing_areas}")
            
            return missing_areas
        finally:
            db.close()
    
    async def scrape_area(self, area: str) -> Dict:
        """æŠ“å–å•ä¸ªåŒºåŸŸçš„æ•°æ®"""
        try:
            print(f"\nğŸŒ å¼€å§‹æŠ“å–åŒºåŸŸ: {area}")
            
            # æ„å»ºAPIè¯·æ±‚
            url = f"http://localhost:8000/api/scraper/scrape/amap"
            params = {"area": area}
            
            print(f"ğŸ“¡ è¯·æ±‚URL: {url}")
            print(f"ğŸ“‹ å‚æ•°: {params}")
            
            # å‘é€è¯·æ±‚
            response = self.session.post(url, params=params, timeout=60)
            
            if response.status_code == 200:
                result = response.json()
                print(f"âœ… æŠ“å–æˆåŠŸ: {result.get('message', '')}")
                return result
            else:
                print(f"âŒ æŠ“å–å¤±è´¥: {response.status_code}")
                return {"error": f"HTTP {response.status_code}"}
                
        except Exception as e:
            print(f"âŒ æŠ“å–å¼‚å¸¸: {e}")
            return {"error": str(e)}
    
    async def scrape_all_missing_areas(self):
        """æŠ“å–æ‰€æœ‰ç¼ºå¤±çš„åŒºåŸŸ"""
        print("ğŸš€ å¼€å§‹æŠ“å–æ‰€æœ‰ç¼ºå¤±åŒºåŸŸ")
        print("=" * 80)
        
        # è·å–ç¼ºå¤±çš„åŒºåŸŸ
        missing_areas = self.get_missing_areas()
        
        if not missing_areas:
            print("âœ… æ‰€æœ‰åŒºåŸŸéƒ½å·²æŠ“å–å®Œæˆï¼")
            return
        
        print(f"ğŸ“‹ éœ€è¦æŠ“å–çš„åŒºåŸŸ: {missing_areas}")
        print(f"ğŸ“Š é¢„è®¡æ–°å¢åœºé¦†æ•°: {len(missing_areas) * 50} (æ¯ä¸ªåŒºåŸŸçº¦50ä¸ªåœºé¦†)")
        print("=" * 80)
        
        # é€ä¸ªæŠ“å–åŒºåŸŸ
        results = {}
        for i, area in enumerate(missing_areas, 1):
            print(f"\nğŸ”„ è¿›åº¦: {i}/{len(missing_areas)}")
            
            result = await self.scrape_area(area)
            results[area] = result
            
            # æ£€æŸ¥ç»“æœ
            if "error" not in result:
                print(f"âœ… {area} æŠ“å–æˆåŠŸ")
            else:
                print(f"âŒ {area} æŠ“å–å¤±è´¥: {result['error']}")
            
            # ç­‰å¾…ä¸€æ®µæ—¶é—´å†æŠ“å–ä¸‹ä¸€ä¸ªåŒºåŸŸ
            if i < len(missing_areas):
                print("â³ ç­‰å¾…5ç§’åç»§ç»­...")
                await asyncio.sleep(5)
        
        # æ€»ç»“æŠ¥å‘Š
        print("\n" + "=" * 80)
        print("ğŸ“Š æŠ“å–æ€»ç»“æŠ¥å‘Š")
        print("=" * 80)
        
        success_count = sum(1 for result in results.values() if "error" not in result)
        failed_count = len(results) - success_count
        
        print(f"âœ… æˆåŠŸæŠ“å–: {success_count} ä¸ªåŒºåŸŸ")
        print(f"âŒ æŠ“å–å¤±è´¥: {failed_count} ä¸ªåŒºåŸŸ")
        
        if failed_count > 0:
            print("\nâŒ å¤±è´¥çš„åŒºåŸŸ:")
            for area, result in results.items():
                if "error" in result:
                    print(f"  - {area}: {result['error']}")
        
        # æ£€æŸ¥æœ€ç»ˆåœºé¦†æ•°é‡
        await self.check_final_stats()
    
    async def check_final_stats(self):
        """æ£€æŸ¥æœ€ç»ˆçš„ç»Ÿè®¡æ•°æ®"""
        print("\n" + "=" * 80)
        print("ğŸ“ˆ æœ€ç»ˆç»Ÿè®¡")
        print("=" * 80)
        
        db = SessionLocal()
        try:
            total_courts = db.query(TennisCourt).count()
            area_stats = {}
            
            for area in settings.target_areas:
                count = db.query(TennisCourt).filter(TennisCourt.area == area).count()
                area_stats[area] = count
            
            print(f"ğŸŸï¸  æ€»åœºé¦†æ•°: {total_courts}")
            
            if total_courts >= 500:
                print(f"ğŸ‰ ç›®æ ‡è¾¾æˆï¼åœºé¦†æ•°é‡: {total_courts}")
            else:
                print(f"âš ï¸  è¿˜éœ€åŠªåŠ›ï¼Œå½“å‰åœºé¦†æ•°: {total_courts}/500")
            
            print("\nğŸ“ å„åŒºåŸŸåˆ†å¸ƒ:")
            for area, count in area_stats.items():
                print(f"  {area}: {count}ä¸ªåœºé¦†")
                
        finally:
            db.close()

async def main():
    """ä¸»å‡½æ•°"""
    scraper = MissingAreasScraper()
    await scraper.scrape_all_missing_areas()

if __name__ == "__main__":
    asyncio.run(main()) 