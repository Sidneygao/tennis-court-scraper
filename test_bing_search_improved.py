#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ”¹è¿›çš„Bingæœç´¢æµ‹è¯•è„šæœ¬
ä½¿ç”¨æ›´ç²¾ç¡®çš„æœç´¢ç­–ç•¥å’ŒHTMLè§£æ
"""

import requests
import time
import json
from urllib.parse import quote
import re
from typing import List, Dict, Optional
from bs4 import BeautifulSoup

class ImprovedBingSearchTester:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
    def search_bing_html(self, query: str, max_results: int = 10) -> List[Dict]:
        """
        ä½¿ç”¨Bingæœç´¢HTMLé¡µé¢å¹¶æå–ç»“æœ
        """
        try:
            # æ„å»ºæœç´¢URL - ä¸ä½¿ç”¨RSSæ ¼å¼
            encoded_query = quote(query)
            url = f"https://www.bing.com/search?q={encoded_query}&count={max_results}"
            
            print(f"ğŸ” æœç´¢æŸ¥è¯¢: {query}")
            print(f"ğŸ“¡ è¯·æ±‚URL: {url}")
            
            # å‘é€è¯·æ±‚
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            
            print(f"âœ… å“åº”çŠ¶æ€: {response.status_code}")
            print(f"ğŸ“„ å“åº”é•¿åº¦: {len(response.text)} å­—ç¬¦")
            
            # è§£æHTMLç»“æœ
            results = self._parse_html_response(response.text)
            
            print(f"ğŸ“Š æå–åˆ° {len(results)} ä¸ªç»“æœ")
            return results[:max_results]
            
        except requests.exceptions.RequestException as e:
            print(f"âŒ è¯·æ±‚é”™è¯¯: {e}")
            return []
        except Exception as e:
            print(f"âŒ è§£æé”™è¯¯: {e}")
            return []
    
    def _parse_html_response(self, content: str) -> List[Dict]:
        """
        è§£æBing HTMLå“åº”
        """
        results = []
        
        try:
            soup = BeautifulSoup(content, 'html.parser')
            
            # æŸ¥æ‰¾æœç´¢ç»“æœ
            search_results = soup.find_all('li', class_='b_algo')
            
            for result in search_results:
                try:
                    # æå–æ ‡é¢˜å’Œé“¾æ¥
                    title_elem = result.find('h2')
                    if title_elem:
                        link_elem = title_elem.find('a')
                        if link_elem:
                            title = link_elem.get_text(strip=True)
                            url = link_elem.get('href', '')
                            
                            # æå–æè¿°
                            desc_elem = result.find('p')
                            description = desc_elem.get_text(strip=True) if desc_elem else ''
                            
                            if title and url:
                                result_data = {
                                    'title': title,
                                    'url': url,
                                    'description': description
                                }
                                results.append(result_data)
                                
                except Exception as e:
                    print(f"âš ï¸ è§£æå•ä¸ªç»“æœæ—¶å‡ºé”™: {e}")
                    continue
            
            # å¦‚æœä¸Šé¢çš„æ–¹æ³•æ²¡æœ‰æ‰¾åˆ°ç»“æœï¼Œå°è¯•å…¶ä»–é€‰æ‹©å™¨
            if not results:
                print("ğŸ” å°è¯•å¤‡ç”¨è§£ææ–¹æ³•...")
                results = self._parse_html_fallback(content)
                
        except Exception as e:
            print(f"âŒ HTMLè§£æé”™è¯¯: {e}")
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ
            results = self._parse_html_regex(content)
        
        return results
    
    def _parse_html_fallback(self, content: str) -> List[Dict]:
        """
        å¤‡ç”¨HTMLè§£ææ–¹æ³•
        """
        results = []
        
        # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„æœç´¢ç»“æœé“¾æ¥
        link_pattern = r'<h2[^>]*><a[^>]*href="([^"]*)"[^>]*>([^<]*)</a></h2>'
        desc_pattern = r'<p[^>]*>([^<]*)</p>'
        
        links = re.findall(link_pattern, content)
        descriptions = re.findall(desc_pattern, content)
        
        for i, (url, title) in enumerate(links):
            if i < len(descriptions):
                description = descriptions[i]
            else:
                description = ""
            
            if title.strip() and url.startswith('http'):
                results.append({
                    'title': title.strip(),
                    'url': url,
                    'description': description.strip()
                })
        
        return results
    
    def _parse_html_regex(self, content: str) -> List[Dict]:
        """
        ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è§£æHTML
        """
        results = []
        
        # æ›´ç®€å•çš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
        pattern = r'<a[^>]*href="([^"]*)"[^>]*>([^<]*)</a>'
        matches = re.findall(pattern, content)
        
        for url, title in matches:
            if (url.startswith('http') and 
                len(title.strip()) > 10 and 
                not title.strip().startswith('http')):
                results.append({
                    'title': title.strip(),
                    'url': url,
                    'description': ''
                })
        
        return results[:10]  # é™åˆ¶ç»“æœæ•°é‡
    
    def test_price_search_improved(self, court_name: str) -> List[Dict]:
        """
        æ”¹è¿›çš„ä»·æ ¼æœç´¢æµ‹è¯•
        """
        queries = [
            f'"{court_name}" åŒ—äº¬ ç½‘çƒåœº ä»·æ ¼ æ”¶è´¹',
            f'"{court_name}" åŒ—äº¬ ç½‘çƒ è´¹ç”¨ ä»·æ ¼è¡¨',
            f'"{court_name}" ç½‘çƒåœº é¢„çº¦ ä»·æ ¼',
            f'"{court_name}" ç½‘çƒ åœºåœ° æ”¶è´¹',
        ]
        
        all_results = []
        
        for query in queries:
            print(f"\n{'='*60}")
            print(f"ğŸ” æµ‹è¯•æŸ¥è¯¢: {query}")
            print(f"{'='*60}")
            
            results = self.search_bing_html(query, max_results=5)
            
            if results:
                print("ğŸ“‹ æœç´¢ç»“æœ:")
                for i, result in enumerate(results, 1):
                    print(f"  {i}. {result['title']}")
                    print(f"     URL: {result['url']}")
                    if result['description']:
                        print(f"     æè¿°: {result['description'][:150]}...")
                    print()
                
                all_results.extend(results)
            else:
                print("âŒ æœªæ‰¾åˆ°ç»“æœ")
            
            # é¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(3)
        
        return all_results
    
    def test_contact_search_improved(self, court_name: str) -> List[Dict]:
        """
        æ”¹è¿›çš„è”ç³»æ–¹å¼æœç´¢æµ‹è¯•
        """
        queries = [
            f'"{court_name}" åŒ—äº¬ ç½‘çƒåœº ç”µè¯ è”ç³»æ–¹å¼',
            f'"{court_name}" ç½‘çƒ é¢„çº¦ ç”µè¯',
            f'"{court_name}" ç½‘çƒåœº è”ç³» å’¨è¯¢',
        ]
        
        all_results = []
        
        for query in queries:
            print(f"\n{'='*60}")
            print(f"ğŸ” æµ‹è¯•æŸ¥è¯¢: {query}")
            print(f"{'='*60}")
            
            results = self.search_bing_html(query, max_results=5)
            
            if results:
                print("ğŸ“‹ æœç´¢ç»“æœ:")
                for i, result in enumerate(results, 1):
                    print(f"  {i}. {result['title']}")
                    print(f"     URL: {result['url']}")
                    if result['description']:
                        print(f"     æè¿°: {result['description'][:150]}...")
                    print()
                
                all_results.extend(results)
            else:
                print("âŒ æœªæ‰¾åˆ°ç»“æœ")
            
            time.sleep(3)
        
        return all_results
    
    def test_specific_sources(self, court_name: str) -> List[Dict]:
        """
        æµ‹è¯•ç‰¹å®šæ¥æºçš„æœç´¢
        """
        sources = [
            f'site:baidu.com "{court_name}" ç½‘çƒåœº ä»·æ ¼',
            f'site:zhihu.com "{court_name}" ç½‘çƒ',
            f'site:weibo.com "{court_name}" ç½‘çƒåœº',
            f'site:dianping.com "{court_name}" ç½‘çƒ',
        ]
        
        all_results = []
        
        for query in sources:
            print(f"\n{'='*60}")
            print(f"ğŸ” ç‰¹å®šæ¥æºæŸ¥è¯¢: {query}")
            print(f"{'='*60}")
            
            results = self.search_bing_html(query, max_results=3)
            
            if results:
                print("ğŸ“‹ æœç´¢ç»“æœ:")
                for i, result in enumerate(results, 1):
                    print(f"  {i}. {result['title']}")
                    print(f"     URL: {result['url']}")
                    if result['description']:
                        print(f"     æè¿°: {result['description'][:150]}...")
                    print()
                
                all_results.extend(results)
            else:
                print("âŒ æœªæ‰¾åˆ°ç»“æœ")
            
            time.sleep(3)
        
        return all_results

def main():
    """
    ä¸»æµ‹è¯•å‡½æ•°
    """
    print("ğŸ¾ æ”¹è¿›çš„Bingæœç´¢æµ‹è¯•å·¥å…·")
    print("=" * 60)
    
    tester = ImprovedBingSearchTester()
    
    # æµ‹è¯•åœºé¦†åˆ—è¡¨
    test_courts = [
        "æœé˜³å…¬å›­ç½‘çƒåœº",
        "é‡‘åœ°ç½‘çƒ",
        "å˜‰é‡Œä¸­å¿ƒç½‘çƒåœº"
    ]
    
    for court in test_courts:
        print(f"\n{'ğŸš€'*25} æµ‹è¯•åœºé¦†: {court} {'ğŸš€'*25}")
        
        # æµ‹è¯•ç‰¹å®šæ¥æºæœç´¢
        print(f"\nğŸ¯ ç‰¹å®šæ¥æºæœç´¢æµ‹è¯•:")
        source_results = tester.test_specific_sources(court)
        
        # æµ‹è¯•æ”¹è¿›çš„ä»·æ ¼æœç´¢
        print(f"\nğŸ’° æ”¹è¿›ä»·æ ¼æœç´¢æµ‹è¯•:")
        price_results = tester.test_price_search_improved(court)
        
        # æµ‹è¯•æ”¹è¿›çš„è”ç³»æ–¹å¼æœç´¢
        print(f"\nğŸ“ æ”¹è¿›è”ç³»æ–¹å¼æœç´¢æµ‹è¯•:")
        contact_results = tester.test_contact_search_improved(court)
        
        # ä¿å­˜ç»“æœ
        results = {
            'court_name': court,
            'source_results': source_results,
            'price_results': price_results,
            'contact_results': contact_results,
            'total_results': len(source_results) + len(price_results) + len(contact_results)
        }
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        filename = f"bing_improved_test_{court.replace(' ', '_')}.json"
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {filename}")
        print(f"ğŸ“Š æ€»è®¡æ‰¾åˆ° {results['total_results']} ä¸ªç»“æœ")
        
        print(f"\n{'='*70}\n")

if __name__ == "__main__":
    main() 