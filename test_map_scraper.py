#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æµ‹è¯•åœ°å›¾çˆ¬è™«åŠŸèƒ½
"""

from app.scrapers.detail_scraper import DetailScraper
import json

def test_map_scrapers():
    """æµ‹è¯•åœ°å›¾çˆ¬è™«"""
    scraper = DetailScraper()
    
    # æµ‹è¯•åœºé¦†åç§°
    test_courts = [
        "çƒæ˜Ÿç½‘çƒæ±‡(åˆç”Ÿæ±‡çƒæ˜Ÿè¿åŠ¨ä¸­å¿ƒåº—)",
        "SOLOTennisç½‘çƒä¿±ä¹éƒ¨",
        "åŠ¨ä¹‹å…‰Â·å¤§æœ›è·¯ç½‘çƒé¦†"
    ]
    
    for court_name in test_courts:
        print(f"\n{'='*60}")
        print(f"æµ‹è¯•åœºé¦†: {court_name}")
        print(f"{'='*60}")
        
        # æµ‹è¯•ç™¾åº¦åœ°å›¾çˆ¬è™«
        print("\nğŸ” æµ‹è¯•ç™¾åº¦åœ°å›¾çˆ¬è™«...")
        try:
            baidu_data = scraper.scrape_baidu_map(court_name)
            if baidu_data:
                print("âœ… ç™¾åº¦åœ°å›¾æ•°æ®è·å–æˆåŠŸ:")
                print(f"   è¯„åˆ†: {baidu_data.get('rating')}")
                print(f"   è¯„è®ºæ•°: {baidu_data.get('review_count')}")
                print(f"   è¯„è®º: {len(baidu_data.get('reviews', []))} æ¡")
                print(f"   è¥ä¸šæ—¶é—´: {baidu_data.get('business_hours')}")
            else:
                print("âŒ ç™¾åº¦åœ°å›¾æ•°æ®è·å–å¤±è´¥")
        except Exception as e:
            print(f"âŒ ç™¾åº¦åœ°å›¾çˆ¬è™«å¼‚å¸¸: {e}")
        
        # æµ‹è¯•é«˜å¾·åœ°å›¾çˆ¬è™«
        print("\nğŸ” æµ‹è¯•é«˜å¾·åœ°å›¾çˆ¬è™«...")
        try:
            amap_data = scraper.scrape_amap(court_name)
            if amap_data:
                print("âœ… é«˜å¾·åœ°å›¾æ•°æ®è·å–æˆåŠŸ:")
                print(f"   è¯„åˆ†: {amap_data.get('rating')}")
                print(f"   è¯„è®ºæ•°: {amap_data.get('review_count')}")
                print(f"   è¯„è®º: {len(amap_data.get('reviews', []))} æ¡")
                print(f"   è¥ä¸šæ—¶é—´: {amap_data.get('business_hours')}")
            else:
                print("âŒ é«˜å¾·åœ°å›¾æ•°æ®è·å–å¤±è´¥")
        except Exception as e:
            print(f"âŒ é«˜å¾·åœ°å›¾çˆ¬è™«å¼‚å¸¸: {e}")
        
        # æµ‹è¯•å°çº¢ä¹¦çˆ¬è™«
        print("\nğŸ” æµ‹è¯•å°çº¢ä¹¦çˆ¬è™«...")
        try:
            xiaohongshu_data = scraper.scrape_xiaohongshu(court_name)
            if xiaohongshu_data:
                print("âœ… å°çº¢ä¹¦æ•°æ®è·å–æˆåŠŸ:")
                print(f"   è¯„åˆ†: {xiaohongshu_data.get('rating')}")
                print(f"   è¯„è®ºæ•°: {xiaohongshu_data.get('review_count')}")
                print(f"   è¯„è®º: {len(xiaohongshu_data.get('reviews', []))} æ¡")
                print(f"   è¥ä¸šæ—¶é—´: {xiaohongshu_data.get('business_hours')}")
                print(f"   ä»·æ ¼: {len(xiaohongshu_data.get('prices', []))} é¡¹")
                print(f"   å›¾ç‰‡: {len(xiaohongshu_data.get('images', []))} å¼ ")
            else:
                print("âŒ å°çº¢ä¹¦æ•°æ®è·å–å¤±è´¥")
        except Exception as e:
            print(f"âŒ å°çº¢ä¹¦çˆ¬è™«å¼‚å¸¸: {e}")
        
        # æµ‹è¯•ç»¼åˆçˆ¬å–
        print("\nğŸ” æµ‹è¯•ç»¼åˆçˆ¬å–...")
        try:
            merged_data = scraper.scrape_court_details(court_name)
            print("âœ… ç»¼åˆæ•°æ®è·å–æˆåŠŸ:")
            print(f"   æè¿°: {merged_data.get('description', '')[:50]}...")
            print(f"   è®¾æ–½: {merged_data.get('facilities', '')[:50]}...")
            print(f"   è¥ä¸šæ—¶é—´: {merged_data.get('business_hours')}")
            print(f"   è¯„åˆ†: {merged_data.get('rating')}")
            print(f"   ä»·æ ¼: {len(merged_data.get('prices', []))} é¡¹")
            print(f"   è¯„è®º: {len(merged_data.get('reviews', []))} æ¡")
            print(f"   å›¾ç‰‡: {len(merged_data.get('images', []))} å¼ ")
        except Exception as e:
            print(f"âŒ ç»¼åˆçˆ¬å–å¼‚å¸¸: {e}")
        
        print("\n" + "-"*60)

if __name__ == "__main__":
    test_map_scrapers() 