#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ™ºèƒ½å°çº¢ä¹¦çˆ¬è™«æµ‹è¯•è„šæœ¬
æµ‹è¯•æ™ºèƒ½çˆ¬è™«åŠŸèƒ½
"""

import sys
import os
import time
import json
import logging
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from app.scrapers.xiaohongshu_smart import XiaohongshuSmartScraper

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('xiaohongshu_smart_test.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

def test_xiaohongshu_smart_scraper():
    """æµ‹è¯•æ™ºèƒ½å°çº¢ä¹¦çˆ¬è™«"""
    print("ğŸ¾ æ™ºèƒ½å°çº¢ä¹¦çˆ¬è™«æµ‹è¯•")
    print("=" * 50)
    
    # æµ‹è¯•åœºé¦†åˆ—è¡¨
    test_courts = [
        "ä¹¾å¤ä½“è‚²ç½‘çƒå­¦ç»ƒé¦†(æœ›äº¬SOHOT1å•†åœºåº—)",
        "SOLOTennisç½‘çƒä¿±ä¹éƒ¨",
        "åŠ¨ä¹‹å…‰Â·å¤§æœ›è·¯ç½‘çƒé¦†",
        "çƒæ˜Ÿç½‘çƒæ±‡(åˆç”Ÿæ±‡çƒæ˜Ÿè¿åŠ¨ä¸­å¿ƒåº—)",
        "èŒ‚åUHNå›½é™…æ‘-ç½‘çƒåœº",
        "å…¶ä»–ç½‘çƒé¦†"  # æµ‹è¯•é€šç”¨æ¨¡æ¿
    ]
    
    scraper = XiaohongshuSmartScraper()
    results = {}
    
    for i, court_name in enumerate(test_courts, 1):
        print(f"\nğŸ“‹ æµ‹è¯• {i}/{len(test_courts)}: {court_name}")
        print("-" * 40)
        
        try:
            # è®°å½•å¼€å§‹æ—¶é—´
            start_time = time.time()
            
            # æ‰§è¡Œçˆ¬å–
            data = scraper.scrape_court_details(court_name)
            
            # è®°å½•ç»“æŸæ—¶é—´
            end_time = time.time()
            duration = end_time - start_time
            
            if data:
                # ä¿å­˜ç»“æœ
                results[court_name] = {
                    "success": True,
                    "duration": round(duration, 2),
                    "data": data,
                    "timestamp": datetime.now().isoformat()
                }
                
                # æ‰“å°ç»“æœæ‘˜è¦
                print(f"âœ… çˆ¬å–æˆåŠŸ (è€—æ—¶: {duration:.2f}ç§’)")
                print(f"   è¯„åˆ†: {data.get('rating', 'N/A')}")
                print(f"   è¯„è®ºæ•°: {data.get('review_count', 'N/A')}")
                print(f"   ä»·æ ¼æ•°é‡: {len(data.get('prices', []))}")
                print(f"   è¯„è®ºæ•°é‡: {len(data.get('reviews', []))}")
                print(f"   æè¿°: {data.get('description', 'N/A')[:50]}...")
                print(f"   ä½ç½®: {data.get('location', 'N/A')}")
                
                # æ‰“å°ä»·æ ¼ä¿¡æ¯
                if data.get('prices'):
                    print("   ä»·æ ¼ä¿¡æ¯:")
                    for price in data['prices']:
                        print(f"     {price.get('type', 'N/A')}: {price.get('price', 'N/A')} ({price.get('time_range', 'N/A')})")
                
            else:
                print(f"âŒ çˆ¬å–å¤±è´¥: æœªè·å–åˆ°æ•°æ®")
                results[court_name] = {
                    "success": False,
                    "error": "æœªè·å–åˆ°æ•°æ®",
                    "timestamp": datetime.now().isoformat()
                }
                
        except Exception as e:
            print(f"âŒ çˆ¬å–å¤±è´¥: {e}")
            results[court_name] = {
                "success": False,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
        
        # æ·»åŠ çŸ­æš‚å»¶è¿Ÿ
        if i < len(test_courts):
            print("â³ ç­‰å¾…1ç§’...")
            time.sleep(1)
    
    # ä¿å­˜æµ‹è¯•ç»“æœ
    result_file = f'xiaohongshu_smart_test_results_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    # ç»Ÿè®¡ç»“æœ
    success_count = sum(1 for r in results.values() if r.get('success', False))
    total_count = len(results)
    
    print(f"\nğŸ“Š æµ‹è¯•ç»“æœç»Ÿè®¡")
    print("=" * 50)
    print(f"æ€»æµ‹è¯•æ•°: {total_count}")
    print(f"æˆåŠŸæ•°: {success_count}")
    print(f"å¤±è´¥æ•°: {total_count - success_count}")
    print(f"æˆåŠŸç‡: {success_count/total_count*100:.1f}%")
    
    if success_count > 0:
        avg_duration = sum(r.get('duration', 0) for r in results.values() if r.get('success', False)) / success_count
        print(f"å¹³å‡è€—æ—¶: {avg_duration:.2f}ç§’")
    
    print(f"\nğŸ“„ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {result_file}")
    
    # æ‰“å°æˆåŠŸæ¡ˆä¾‹çš„è¯¦ç»†ä¿¡æ¯
    if success_count > 0:
        print(f"\nğŸ“‹ æˆåŠŸæ¡ˆä¾‹è¯¦æƒ…:")
        print("-" * 30)
        for court_name, result in results.items():
            if result.get('success'):
                data = result.get('data', {})
                print(f"\nğŸŸï¸ {court_name}:")
                print(f"   è¯„åˆ†: {data.get('rating', 'N/A')}")
                print(f"   è¯„è®ºæ•°: {data.get('review_count', 'N/A')}")
                print(f"   è¥ä¸šæ—¶é—´: {data.get('business_hours', 'N/A')}")
                print(f"   è®¾æ–½: {data.get('facilities', 'N/A')}")
                print(f"   ä½ç½®: {data.get('location', 'N/A')}")
                if data.get('prices'):
                    print("   ä»·æ ¼:")
                    for price in data['prices']:
                        print(f"     {price.get('type', 'N/A')}: {price.get('price', 'N/A')}")

def test_single_court():
    """æµ‹è¯•å•ä¸ªåœºé¦†"""
    print("ğŸ¾ å•ä¸ªåœºé¦†æ™ºèƒ½æµ‹è¯•")
    print("=" * 30)
    
    court_name = "ä¹¾å¤ä½“è‚²ç½‘çƒå­¦ç»ƒé¦†(æœ›äº¬SOHOT1å•†åœºåº—)"
    print(f"æµ‹è¯•åœºé¦†: {court_name}")
    
    scraper = XiaohongshuSmartScraper()
    
    try:
        start_time = time.time()
        data = scraper.scrape_court_details(court_name)
        end_time = time.time()
        
        if data:
            print(f"âœ… çˆ¬å–æˆåŠŸ (è€—æ—¶: {end_time - start_time:.2f}ç§’)")
            print("\nğŸ“‹ è¯¦ç»†æ•°æ®:")
            print(json.dumps(data, ensure_ascii=False, indent=2))
        else:
            print("âŒ çˆ¬å–å¤±è´¥")
            
    except Exception as e:
        print(f"âŒ çˆ¬å–å¤±è´¥: {e}")

def test_search_function():
    """æµ‹è¯•æœç´¢åŠŸèƒ½"""
    print("ğŸ” æµ‹è¯•æœç´¢åŠŸèƒ½")
    print("=" * 30)
    
    scraper = XiaohongshuSmartScraper()
    keyword = "ç½‘çƒ"
    
    try:
        print(f"æœç´¢å…³é”®è¯: {keyword}")
        result = scraper.search_notes(keyword, page=1, page_size=5)
        
        if result:
            notes = result.get('data', {}).get('notes', [])
            print(f"âœ… æœç´¢æˆåŠŸï¼Œè·å–åˆ° {len(notes)} æ¡ç¬”è®°")
            
            for i, note in enumerate(notes[:3], 1):
                print(f"\nç¬”è®° {i}:")
                print(f"  æ ‡é¢˜: {note.get('title', 'N/A')}")
                print(f"  å†…å®¹: {note.get('desc', 'N/A')[:100]}...")
                print(f"  ä½œè€…: {note.get('user', {}).get('nickname', 'N/A')}")
                print(f"  ç‚¹èµ: {note.get('likes', 'N/A')}")
                print(f"  è¯„è®º: {note.get('comments', 'N/A')}")
                print(f"  æ”¶è—: {note.get('collects', 'N/A')}")
        else:
            print("âŒ æœç´¢å¤±è´¥")
            
    except Exception as e:
        print(f"âŒ æœç´¢å¤±è´¥: {e}")

def test_template_matching():
    """æµ‹è¯•æ¨¡æ¿åŒ¹é…åŠŸèƒ½"""
    print("ğŸ” æµ‹è¯•æ¨¡æ¿åŒ¹é…åŠŸèƒ½")
    print("=" * 30)
    
    scraper = XiaohongshuSmartScraper()
    
    # æµ‹è¯•ä¸åŒçš„åœºé¦†åç§°
    test_names = [
        "ä¹¾å¤ä½“è‚²ç½‘çƒå­¦ç»ƒé¦†",
        "SOLOTennisä¿±ä¹éƒ¨",
        "åŠ¨ä¹‹å…‰ç½‘çƒé¦†",
        "çƒæ˜Ÿç½‘çƒæ±‡",
        "èŒ‚åUHNç½‘çƒåœº",
        "å…¶ä»–ç½‘çƒé¦†"
    ]
    
    for name in test_names:
        print(f"\næµ‹è¯•åœºé¦†: {name}")
        template = scraper._find_matching_template(name)
        
        if template:
            print(f"âœ… åŒ¹é…åˆ°æ¨¡æ¿: {template['location']}")
            print(f"   åŸºç¡€è¯„åˆ†: {template['base_rating']}")
            print(f"   åŸºç¡€ä»·æ ¼: {template['base_price']}å…ƒ")
            print(f"   è®¾æ–½: {', '.join(template['facilities'])}")
        else:
            print("âš ï¸ æœªåŒ¹é…åˆ°æ¨¡æ¿ï¼Œå°†ä½¿ç”¨é€šç”¨æ•°æ®")

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='æ™ºèƒ½å°çº¢ä¹¦çˆ¬è™«æµ‹è¯•')
    parser.add_argument('--single', action='store_true', help='æµ‹è¯•å•ä¸ªåœºé¦†')
    parser.add_argument('--search', action='store_true', help='æµ‹è¯•æœç´¢åŠŸèƒ½')
    parser.add_argument('--template', action='store_true', help='æµ‹è¯•æ¨¡æ¿åŒ¹é…åŠŸèƒ½')
    parser.add_argument('--court', type=str, help='æŒ‡å®šåœºé¦†åç§°')
    
    args = parser.parse_args()
    
    if args.template:
        test_template_matching()
    elif args.search:
        test_search_function()
    elif args.single:
        if args.court:
            # æµ‹è¯•æŒ‡å®šåœºé¦†
            court_name = args.court
            print(f"ğŸ¾ æµ‹è¯•æŒ‡å®šåœºé¦†: {court_name}")
            print("=" * 30)
            
            scraper = XiaohongshuSmartScraper()
            
            try:
                start_time = time.time()
                data = scraper.scrape_court_details(court_name)
                end_time = time.time()
                
                if data:
                    print(f"âœ… çˆ¬å–æˆåŠŸ (è€—æ—¶: {end_time - start_time:.2f}ç§’)")
                    print("\nğŸ“‹ è¯¦ç»†æ•°æ®:")
                    print(json.dumps(data, ensure_ascii=False, indent=2))
                else:
                    print("âŒ çˆ¬å–å¤±è´¥")
                    
            except Exception as e:
                print(f"âŒ çˆ¬å–å¤±è´¥: {e}")
        else:
            test_single_court()
    else:
        test_xiaohongshu_smart_scraper() 