#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Google Monica AI é£æ ¼å…³é”®è¯æœç´¢æµ‹è¯•
æ”¯æŒGoogleæœç´¢ï¼ŒæŠ“å–æœç´¢ç»“æœå¿«ç…§å’Œå‰5ä¸ªç»“æœé¡µé¢
"""

import requests
import time
import json
import re
from bs4 import BeautifulSoup
from urllib.parse import quote, urljoin, urlparse
import random
from datetime import datetime
import os

class GoogleMonicaAISearch:
    def __init__(self):
        self.session = requests.Session()
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0'
        ]
        
        self.session.headers.update({
            'User-Agent': random.choice(self.user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        })
        
        # ä»·æ ¼æå–æ­£åˆ™è¡¨è¾¾å¼
        self.price_patterns = [
            r'(\d+)\s*å…ƒ/?å°æ—¶',
            r'(\d+)\s*å…ƒ/?åœº',
            r'ä»·æ ¼[ï¼š:]\s*(\d+)\s*å…ƒ',
            r'æ”¶è´¹[ï¼š:]\s*(\d+)\s*å…ƒ',
            r'å¹³æ—¥ä»·[ï¼š:]\s*(\d+)\s*å…ƒ',
            r'å‘¨æœ«ä»·[ï¼š:]\s*(\d+)\s*å…ƒ',
            r'(\d+)\s*-\s*(\d+)\s*å…ƒ',
            r'(\d+)\s*åˆ°\s*(\d+)\s*å…ƒ',
            r'ï¿¥(\d+)',
            r'(\d+)\s*å—',
            r'(\d+)\s*å…ƒèµ·',
            r'(\d+)\s*å…ƒ/äºº',
            r'(\d+)\s*å…ƒ/æ¬¡'
        ]
    
    def search_google(self, keyword, max_results=5):
        """Googleæœç´¢ï¼Œè¿”å›æœç´¢ç»“æœ"""
        print(f"ğŸ” Googleæœç´¢: {keyword}")
        
        # æ„å»ºGoogleæœç´¢URL
        search_url = f"https://www.google.com/search?q={quote(keyword)}&num={max_results}&hl=zh-CN"
        
        try:
            # æ·»åŠ éšæœºå»¶è¿Ÿ
            time.sleep(random.uniform(1, 3))
            
            response = self.session.get(search_url, timeout=10)
            response.raise_for_status()
            
            print(f"âœ… Googleæœç´¢æˆåŠŸï¼ŒçŠ¶æ€ç : {response.status_code}")
            return response.text
            
        except requests.exceptions.RequestException as e:
            print(f"âŒ Googleæœç´¢å¤±è´¥: {e}")
            return None
    
    def parse_google_results(self, html_content):
        """è§£æGoogleæœç´¢ç»“æœ"""
        if not html_content:
            return []
        
        soup = BeautifulSoup(html_content, 'html.parser')
        results = []
        
        # æŸ¥æ‰¾æœç´¢ç»“æœ
        search_results = soup.find_all('div', class_='g')
        
        for result in search_results[:5]:  # åªå–å‰5ä¸ªç»“æœ
            try:
                # æå–æ ‡é¢˜
                title_elem = result.find('h3')
                title = title_elem.get_text().strip() if title_elem else "æ— æ ‡é¢˜"
                
                # æå–é“¾æ¥
                link_elem = result.find('a')
                url = link_elem.get('href') if link_elem else ""
                
                # æå–æ‘˜è¦
                snippet_elem = result.find('div', class_='VwiC3b')
                snippet = snippet_elem.get_text().strip() if snippet_elem else ""
                
                if title and url and url.startswith('http'):
                    results.append({
                        'title': title,
                        'url': url,
                        'snippet': snippet
                    })
                    
            except Exception as e:
                print(f"âš ï¸ è§£ææœç´¢ç»“æœå‡ºé”™: {e}")
                continue
        
        return results
    
    def extract_prices_from_text(self, text):
        """ä»æ–‡æœ¬ä¸­æå–ä»·æ ¼ä¿¡æ¯"""
        prices = []
        
        for pattern in self.price_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    # å¤„ç†ä»·æ ¼èŒƒå›´
                    if len(match) == 2:
                        prices.append(f"{match[0]}-{match[1]}å…ƒ")
                    else:
                        prices.append(f"{match[0]}å…ƒ")
                else:
                    prices.append(f"{match}å…ƒ")
        
        return list(set(prices))  # å»é‡
    
    def fetch_page_content(self, url):
        """è·å–é¡µé¢å†…å®¹"""
        try:
            # æ·»åŠ éšæœºå»¶è¿Ÿ
            time.sleep(random.uniform(1, 2))
            
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            
            # è®¾ç½®ç¼–ç 
            response.encoding = response.apparent_encoding
            
            return response.text
            
        except requests.exceptions.RequestException as e:
            print(f"âŒ è·å–é¡µé¢å¤±è´¥ {url}: {e}")
            return None
    
    def analyze_page_content(self, url, content):
        """åˆ†æé¡µé¢å†…å®¹ï¼Œæå–ä»·æ ¼ä¿¡æ¯"""
        if not content:
            return []
        
        soup = BeautifulSoup(content, 'html.parser')
        
        # æå–é¡µé¢æ–‡æœ¬
        text = soup.get_text()
        
        # æå–ä»·æ ¼
        prices = self.extract_prices_from_text(text)
        
        # æå–åŒ…å«ä»·æ ¼çš„ä¸Šä¸‹æ–‡
        price_contexts = []
        for pattern in self.price_patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                start = max(0, match.start() - 50)
                end = min(len(text), match.end() + 50)
                context = text[start:end].strip()
                price_contexts.append(context)
        
        return {
            'prices': prices,
            'contexts': price_contexts[:5]  # åªå–å‰5ä¸ªä¸Šä¸‹æ–‡
        }
    
    def search_and_extract(self, keyword):
        """æ‰§è¡Œå®Œæ•´çš„æœç´¢å’Œæå–æµç¨‹"""
        print(f"\nğŸ¤– Google Monica AI é£æ ¼æœç´¢")
        print(f"ğŸ” å…³é”®è¯: {keyword}")
        print("=" * 60)
        
        # 1. Googleæœç´¢
        search_html = self.search_google(keyword)
        if not search_html:
            return None
        
        # 2. è§£ææœç´¢ç»“æœ
        search_results = self.parse_google_results(search_html)
        print(f"ğŸ“Š æ‰¾åˆ° {len(search_results)} ä¸ªæœç´¢ç»“æœ")
        
        # 3. åˆ†ææ¯ä¸ªç»“æœé¡µé¢
        detailed_results = []
        
        for i, result in enumerate(search_results, 1):
            print(f"\nğŸ“„ åˆ†æç¬¬ {i} ä¸ªç»“æœ: {result['title']}")
            print(f"ğŸ”— URL: {result['url']}")
            
            # è·å–é¡µé¢å†…å®¹
            page_content = self.fetch_page_content(result['url'])
            
            # åˆ†æé¡µé¢å†…å®¹
            analysis = self.analyze_page_content(result['url'], page_content)
            
            detailed_result = {
                'rank': i,
                'title': result['title'],
                'url': result['url'],
                'snippet': result['snippet'],
                'prices': analysis['prices'],
                'price_contexts': analysis['contexts']
            }
            
            detailed_results.append(detailed_result)
            
            print(f"ğŸ’° æå–åˆ°ä»·æ ¼: {analysis['prices']}")
        
        # 4. æ±‡æ€»ç»“æœ
        all_prices = []
        for result in detailed_results:
            all_prices.extend(result['prices'])
        
        summary = {
            'keyword': keyword,
            'search_time': datetime.now().isoformat(),
            'total_results': len(detailed_results),
            'total_prices_found': len(all_prices),
            'unique_prices': list(set(all_prices)),
            'detailed_results': detailed_results
        }
        
        return summary

def main():
    """ä¸»å‡½æ•°"""
    searcher = GoogleMonicaAISearch()
    
    # æµ‹è¯•å…³é”®è¯
    test_keywords = [
        "æœé˜³å…¬å›­ç½‘çƒåœº ä»·æ ¼ 2024",
        "é‡‘åœ°ç½‘çƒ ä»·æ ¼ 2024",
        "å˜‰é‡Œä¸­å¿ƒç½‘çƒåœº ä»·æ ¼ 2024"
    ]
    
    all_results = []
    
    for keyword in test_keywords:
        print(f"\n{'='*80}")
        result = searcher.search_and_extract(keyword)
        
        if result:
            all_results.append(result)
            
            # ä¿å­˜å•ä¸ªå…³é”®è¯ç»“æœ
            filename = f"google_monica_{keyword.replace(' ', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {filename}")
    
    # ä¿å­˜æ±‡æ€»ç»“æœ
    summary_filename = f"google_monica_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(summary_filename, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ‰ æµ‹è¯•å®Œæˆï¼æ±‡æ€»ç»“æœå·²ä¿å­˜åˆ°: {summary_filename}")
    
    # æ‰“å°æ±‡æ€»ç»Ÿè®¡
    print(f"\nğŸ“Š æ±‡æ€»ç»Ÿè®¡:")
    for result in all_results:
        print(f"å…³é”®è¯: {result['keyword']}")
        print(f"  æœç´¢ç»“æœ: {result['total_results']} ä¸ª")
        print(f"  æ‰¾åˆ°ä»·æ ¼: {result['total_prices_found']} ä¸ª")
        print(f"  å”¯ä¸€ä»·æ ¼: {result['unique_prices']}")
        print()

if __name__ == "__main__":
    main() 