#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°èŒƒå›´æµ‹è¯•ä¹åŠ¨åŠ›ã€è¶£è¿åŠ¨ã€è±†ç“£ã€çŸ¥ä¹ä»·æ ¼æŠ“å–
"""

import requests
import time
import json
import re
from bs4 import BeautifulSoup
from urllib.parse import quote, urljoin
import random

class PriceScraperTest:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
    def test_ledongli(self, court_name):
        """æµ‹è¯•ä¹åŠ¨åŠ›ä»·æ ¼æŠ“å–"""
        print(f"\nğŸ” æµ‹è¯•ä¹åŠ¨åŠ›: {court_name}")
        try:
            # ä¹åŠ¨åŠ›æœç´¢é¡µé¢
            search_url = f"https://www.ledongli.cn/search?keyword={quote(court_name + 'ç½‘çƒ')}"
            print(f"æœç´¢URL: {search_url}")
            
            response = self.session.get(search_url, timeout=10)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # æŸ¥æ‰¾ä»·æ ¼ç›¸å…³ä¿¡æ¯
                price_elements = soup.find_all(text=re.compile(r'[\d]+å…ƒ|[\d]+/å°æ—¶|[\d]+/åœº'))
                if price_elements:
                    print(f"âœ… æ‰¾åˆ°ä»·æ ¼ä¿¡æ¯: {price_elements[:3]}")
                    return price_elements[:3]
                else:
                    print("âŒ æœªæ‰¾åˆ°ä»·æ ¼ä¿¡æ¯")
                    return None
            else:
                print(f"âŒ è¯·æ±‚å¤±è´¥: {response.status_code}")
                return None
                
        except Exception as e:
            print(f"âŒ ä¹åŠ¨åŠ›æŠ“å–å¼‚å¸¸: {e}")
            return None
    
    def test_quyundong(self, court_name):
        """æµ‹è¯•è¶£è¿åŠ¨ä»·æ ¼æŠ“å–"""
        print(f"\nğŸ” æµ‹è¯•è¶£è¿åŠ¨: {court_name}")
        try:
            # è¶£è¿åŠ¨æœç´¢é¡µé¢
            search_url = f"https://www.quyundong.com/search?q={quote(court_name + 'ç½‘çƒ')}"
            print(f"æœç´¢URL: {search_url}")
            
            response = self.session.get(search_url, timeout=10)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # æŸ¥æ‰¾ä»·æ ¼ç›¸å…³ä¿¡æ¯
                price_elements = soup.find_all(text=re.compile(r'[\d]+å…ƒ|[\d]+/å°æ—¶|[\d]+/åœº'))
                if price_elements:
                    print(f"âœ… æ‰¾åˆ°ä»·æ ¼ä¿¡æ¯: {price_elements[:3]}")
                    return price_elements[:3]
                else:
                    print("âŒ æœªæ‰¾åˆ°ä»·æ ¼ä¿¡æ¯")
                    return None
            else:
                print(f"âŒ è¯·æ±‚å¤±è´¥: {response.status_code}")
                return None
                
        except Exception as e:
            print(f"âŒ è¶£è¿åŠ¨æŠ“å–å¼‚å¸¸: {e}")
            return None
    
    def test_douban(self, court_name):
        """æµ‹è¯•è±†ç“£ä»·æ ¼æŠ“å–"""
        print(f"\nğŸ” æµ‹è¯•è±†ç“£: {court_name}")
        try:
            # è±†ç“£æœç´¢é¡µé¢
            search_url = f"https://www.douban.com/search?cat=1005&q={quote(court_name + 'ç½‘çƒ')}"
            print(f"æœç´¢URL: {search_url}")
            
            response = self.session.get(search_url, timeout=10)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # æŸ¥æ‰¾ä»·æ ¼ç›¸å…³ä¿¡æ¯
                price_elements = soup.find_all(text=re.compile(r'[\d]+å…ƒ|[\d]+/å°æ—¶|[\d]+/åœº'))
                if price_elements:
                    print(f"âœ… æ‰¾åˆ°ä»·æ ¼ä¿¡æ¯: {price_elements[:3]}")
                    return price_elements[:3]
                else:
                    print("âŒ æœªæ‰¾åˆ°ä»·æ ¼ä¿¡æ¯")
                    return None
            else:
                print(f"âŒ è¯·æ±‚å¤±è´¥: {response.status_code}")
                return None
                
        except Exception as e:
            print(f"âŒ è±†ç“£æŠ“å–å¼‚å¸¸: {e}")
            return None
    
    def test_zhihu(self, court_name):
        """æµ‹è¯•çŸ¥ä¹ä»·æ ¼æŠ“å–"""
        print(f"\nğŸ” æµ‹è¯•çŸ¥ä¹: {court_name}")
        try:
            # çŸ¥ä¹æœç´¢é¡µé¢
            search_url = f"https://www.zhihu.com/search?type=content&q={quote(court_name + 'ç½‘çƒä»·æ ¼')}"
            print(f"æœç´¢URL: {search_url}")
            
            response = self.session.get(search_url, timeout=10)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # æŸ¥æ‰¾ä»·æ ¼ç›¸å…³ä¿¡æ¯
                price_elements = soup.find_all(text=re.compile(r'[\d]+å…ƒ|[\d]+/å°æ—¶|[\d]+/åœº'))
                if price_elements:
                    print(f"âœ… æ‰¾åˆ°ä»·æ ¼ä¿¡æ¯: {price_elements[:3]}")
                    return price_elements[:3]
                else:
                    print("âŒ æœªæ‰¾åˆ°ä»·æ ¼ä¿¡æ¯")
                    return None
            else:
                print(f"âŒ è¯·æ±‚å¤±è´¥: {response.status_code}")
                return None
                
        except Exception as e:
            print(f"âŒ çŸ¥ä¹æŠ“å–å¼‚å¸¸: {e}")
            return None
    
    def test_all_platforms(self, court_name):
        """æµ‹è¯•æ‰€æœ‰å¹³å°"""
        print(f"\nğŸ¾ å¼€å§‹æµ‹è¯•åœºé¦†: {court_name}")
        print("=" * 50)
        
        results = {}
        
        # æµ‹è¯•ä¹åŠ¨åŠ›
        results['ledongli'] = self.test_ledongli(court_name)
        time.sleep(random.uniform(1, 3))
        
        # æµ‹è¯•è¶£è¿åŠ¨
        results['quyundong'] = self.test_quyundong(court_name)
        time.sleep(random.uniform(1, 3))
        
        # æµ‹è¯•è±†ç“£
        results['douban'] = self.test_douban(court_name)
        time.sleep(random.uniform(1, 3))
        
        # æµ‹è¯•çŸ¥ä¹
        results['zhihu'] = self.test_zhihu(court_name)
        time.sleep(random.uniform(1, 3))
        
        # æ±‡æ€»ç»“æœ
        print(f"\nğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»:")
        print("=" * 50)
        for platform, result in results.items():
            status = "âœ… æˆåŠŸ" if result else "âŒ å¤±è´¥"
            print(f"{platform:12}: {status}")
            if result:
                print(f"           ä»·æ ¼: {result}")
        
        return results

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¾ ç½‘çƒåœºé¦†ä»·æ ¼æŠ“å–æµ‹è¯•")
    print("=" * 50)
    
    # æµ‹è¯•åœºé¦†åˆ—è¡¨
    test_courts = [
        "å˜‰é‡Œä¸­å¿ƒç½‘çƒåœº",
        "é‡‘åœ°ç½‘çƒä¸­å¿ƒ", 
        "WoowTennisç½‘çƒä¿±ä¹éƒ¨"
    ]
    
    scraper = PriceScraperTest()
    
    all_results = {}
    
    for court in test_courts:
        results = scraper.test_all_platforms(court)
        all_results[court] = results
        print("\n" + "=" * 60 + "\n")
    
    # ä¿å­˜æµ‹è¯•ç»“æœ
    with open('price_test_results.json', 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    
    print("ğŸ“ æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ° price_test_results.json")
    
    # ç»Ÿè®¡æˆåŠŸç‡
    total_tests = len(test_courts) * 4
    successful_tests = sum(1 for court_results in all_results.values() 
                          for result in court_results.values() if result)
    
    print(f"\nğŸ“ˆ æµ‹è¯•ç»Ÿè®¡:")
    print(f"æ€»æµ‹è¯•æ•°: {total_tests}")
    print(f"æˆåŠŸæ•°: {successful_tests}")
    print(f"æˆåŠŸç‡: {successful_tests/total_tests*100:.1f}%")

if __name__ == "__main__":
    main() 